{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSMC_KoBERT_submit",
      "provenance": [],
      "collapsed_sections": [
        "poGtO7k0Vd0S"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPh7rQMxUM93FZ45r51/OsF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heejin0223/NLPHW/blob/main/NSMC_KoBERT_submit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkYDNYi5tnAj"
      },
      "source": [
        "###### 데이터 출처 : https://github.com/e9t/nsmc.git   \r\n",
        "###### SKTBrain KoBERT 활용 : https://github.com/monologg/KoBERT-Transformers\r\n",
        "###### 소스참조 :\r\n",
        "- https://github.com/deepseasw/bert-naver-movie-review\r\n",
        "- https://github.com/kimwoonggon/publicservant_AI\r\n",
        "- https://github.com/monologg/KoBERT-Transformers\r\n",
        "- https://github.com/SKTBrain/KoBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0b6N3Eitjbb"
      },
      "source": [
        "#### setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5rg0MFuUQ69"
      },
      "source": [
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxXNmsCnUbJ-"
      },
      "source": [
        "DATA_PATH = 'gdrive/My Drive/Colab Notebooks/NLP/'\r\n",
        "import sys\r\n",
        "sys.path.append(DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIAoH9KoUkYB"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9dcLG1mUdst"
      },
      "source": [
        "import torch\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from transformers import *\r\n",
        "import json\r\n",
        "from tqdm import tqdm\r\n",
        "import math\r\n",
        "import urllib.request\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVJXhzn1UjHI"
      },
      "source": [
        "if torch.cuda.is_available():    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxClcCvkVJer"
      },
      "source": [
        "#### data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsP6xVwjVHMA"
      },
      "source": [
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2KpdqRLZpiL"
      },
      "source": [
        "os.listdir('nsmc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fSjrRPfVOwL"
      },
      "source": [
        "total = pd.read_csv(\"nsmc/\"+\"ratings_train.txt\", sep='\\t')\r\n",
        "test = pd.read_csv(\"nsmc/\"+\"ratings_test.txt\", sep='\\t')\r\n",
        "print(total.shape)\r\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAsYLj1xVRRL"
      },
      "source": [
        "total[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy7w4TVoc4ch"
      },
      "source": [
        "total.columns = ['id','Sentence','label']\r\n",
        "total[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihWGPJFTJwQr"
      },
      "source": [
        "test.columns = ['id','Sentence','label']\r\n",
        "test[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtbj12ZEG0eM"
      },
      "source": [
        "train, val = train_test_split(total, \r\n",
        "                                    test_size=0.15, \r\n",
        "                                    random_state=42, \r\n",
        "                                    stratify=total.label.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl4s3a9PHYre"
      },
      "source": [
        "train = train.reset_index(drop=True)\r\n",
        "val = val.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKnmAdPCHyKF"
      },
      "source": [
        "print(len(train))\r\n",
        "print(len(val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poGtO7k0Vd0S"
      },
      "source": [
        "#### tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbqvGS_qZx2j"
      },
      "source": [
        "!pip install sentencepiece "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBz2CGqGZ4dC"
      },
      "source": [
        "## https://github.com/monologg/KoBERT-Transformers : Tokenizer설정\r\n",
        "\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "from shutil import copyfile\r\n",
        "\r\n",
        "from transformers import PreTrainedTokenizer\r\n",
        "\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\r\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\r\n",
        "\r\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\r\n",
        "    \"vocab_file\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\r\n",
        "    },\r\n",
        "    \"vocab_txt\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\r\n",
        "    \"monologg/kobert\": 512,\r\n",
        "    \"monologg/kobert-lm\": 512,\r\n",
        "    \"monologg/distilkobert\": 512\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_INIT_CONFIGURATION = {\r\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\r\n",
        "}\r\n",
        "\r\n",
        "SPIECE_UNDERLINE = u'▁'\r\n",
        "\r\n",
        "\r\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\r\n",
        "    \"\"\"\r\n",
        "        SentencePiece based tokenizer. Peculiarities:\r\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\r\n",
        "    \"\"\"\r\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\r\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\r\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\r\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            vocab_file,\r\n",
        "            vocab_txt,\r\n",
        "            do_lower_case=False,\r\n",
        "            remove_space=True,\r\n",
        "            keep_accents=False,\r\n",
        "            unk_token=\"[UNK]\",\r\n",
        "            sep_token=\"[SEP]\",\r\n",
        "            pad_token=\"[PAD]\",\r\n",
        "            cls_token=\"[CLS]\",\r\n",
        "            mask_token=\"[MASK]\",\r\n",
        "            **kwargs):\r\n",
        "        super().__init__(\r\n",
        "            unk_token=unk_token,\r\n",
        "            sep_token=sep_token,\r\n",
        "            pad_token=pad_token,\r\n",
        "            cls_token=cls_token,\r\n",
        "            mask_token=mask_token,\r\n",
        "            **kwargs\r\n",
        "        )\r\n",
        "\r\n",
        "        # Build vocab\r\n",
        "        self.token2idx = dict()\r\n",
        "        self.idx2token = []\r\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\r\n",
        "            for idx, token in enumerate(f):\r\n",
        "                token = token.strip()\r\n",
        "                self.token2idx[token] = idx\r\n",
        "                self.idx2token.append(token)\r\n",
        "\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "\r\n",
        "        self.do_lower_case = do_lower_case\r\n",
        "        self.remove_space = remove_space\r\n",
        "        self.keep_accents = keep_accents\r\n",
        "        self.vocab_file = vocab_file\r\n",
        "        self.vocab_txt = vocab_txt\r\n",
        "\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(vocab_file)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def vocab_size(self):\r\n",
        "        return len(self.idx2token)\r\n",
        "\r\n",
        "    def get_vocab(self):\r\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\r\n",
        "\r\n",
        "    def __getstate__(self):\r\n",
        "        state = self.__dict__.copy()\r\n",
        "        state[\"sp_model\"] = None\r\n",
        "        return state\r\n",
        "\r\n",
        "    def __setstate__(self, d):\r\n",
        "        self.__dict__ = d\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(self.vocab_file)\r\n",
        "\r\n",
        "    def preprocess_text(self, inputs):\r\n",
        "        if self.remove_space:\r\n",
        "            outputs = \" \".join(inputs.strip().split())\r\n",
        "        else:\r\n",
        "            outputs = inputs\r\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\r\n",
        "\r\n",
        "        if not self.keep_accents:\r\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\r\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\r\n",
        "        if self.do_lower_case:\r\n",
        "            outputs = outputs.lower()\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\r\n",
        "        \"\"\" Tokenize a string. \"\"\"\r\n",
        "        text = self.preprocess_text(text)\r\n",
        "\r\n",
        "        if not sample:\r\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\r\n",
        "        else:\r\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\r\n",
        "        new_pieces = []\r\n",
        "        for piece in pieces:\r\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\r\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\r\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\r\n",
        "                    if len(cur_pieces[0]) == 1:\r\n",
        "                        cur_pieces = cur_pieces[1:]\r\n",
        "                    else:\r\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\r\n",
        "                cur_pieces.append(piece[-1])\r\n",
        "                new_pieces.extend(cur_pieces)\r\n",
        "            else:\r\n",
        "                new_pieces.append(piece)\r\n",
        "\r\n",
        "        return new_pieces\r\n",
        "\r\n",
        "    def _convert_token_to_id(self, token):\r\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\r\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\r\n",
        "\r\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\r\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\r\n",
        "        return self.idx2token[index]\r\n",
        "\r\n",
        "    def convert_tokens_to_string(self, tokens):\r\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\r\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\r\n",
        "        return out_string\r\n",
        "\r\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n",
        "        by concatenating and adding special tokens.\r\n",
        "        A KoBERT sequence has the following format:\r\n",
        "            single sequence: [CLS] X [SEP]\r\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\r\n",
        "        \"\"\"\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\r\n",
        "\r\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\r\n",
        "        \"\"\"\r\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\r\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\r\n",
        "        Args:\r\n",
        "            token_ids_0: list of ids (must not contain special tokens)\r\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\r\n",
        "                for sequence pairs\r\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\r\n",
        "                special tokens for the model\r\n",
        "        Returns:\r\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if already_has_special_tokens:\r\n",
        "            if token_ids_1 is not None:\r\n",
        "                raise ValueError(\r\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\r\n",
        "                    \"ids is already formated with special tokens for the model.\"\r\n",
        "                )\r\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\r\n",
        "\r\n",
        "        if token_ids_1 is not None:\r\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\r\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\r\n",
        "\r\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\r\n",
        "        A KoBERT sequence pair mask has the following format:\r\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\r\n",
        "        | first sequence    | second sequence\r\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\r\n",
        "        \"\"\"\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return len(cls + token_ids_0 + sep) * [0]\r\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\r\n",
        "\r\n",
        "    def save_vocabulary(self, save_directory):\r\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\r\n",
        "            to a directory.\r\n",
        "        \"\"\"\r\n",
        "        if not os.path.isdir(save_directory):\r\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\r\n",
        "            return\r\n",
        "\r\n",
        "        # 1. Save sentencepiece model\r\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\r\n",
        "\r\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\r\n",
        "            copyfile(self.vocab_file, out_vocab_model)\r\n",
        "\r\n",
        "        # 2. Save vocab.txt\r\n",
        "        index = 0\r\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\r\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\r\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\r\n",
        "                if index != token_index:\r\n",
        "                    logger.warning(\r\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\r\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\r\n",
        "                    )\r\n",
        "                    index = token_index\r\n",
        "                writer.write(token + \"\\n\")\r\n",
        "                index += 1\r\n",
        "\r\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn0_7wyhZ9Ht"
      },
      "source": [
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgIB8sRdNayd"
      },
      "source": [
        "#### inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRnbIBZTQTaM"
      },
      "source": [
        "SEQ_LEN = 128\r\n",
        "BATCH_SIZE = 32\r\n",
        "EPOCHS = 5\r\n",
        "ITERATIONS = math.ceil(len(train)//BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y34s0WKvVvAS"
      },
      "source": [
        "def convert_data(data_df):\r\n",
        "    global tokenizer\r\n",
        "    \r\n",
        "#    SEQ_LEN = MAX_LEN    \r\n",
        "    \r\n",
        "    tokens, masks, segments, targets = [], [], [], []\r\n",
        "    \r\n",
        "    for i in tqdm(range(len(data_df))):\r\n",
        "        # 토큰 : 문장을 토큰화한 값\r\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\r\n",
        "       \r\n",
        "        # 마스크 : 토큰화한 문장 중 패딩은 0, 문자는 1\r\n",
        "        num_zeros = token.count(0)\r\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\r\n",
        "        \r\n",
        "        # 세그먼트 : 문장의 전후관계 구분 (단일문장은 전후관계 없으므로 모두 0)\r\n",
        "        segment = [0]*SEQ_LEN\r\n",
        "\r\n",
        "        # 버트 인풋값을 각각 tokens,masks, segments에 저장\r\n",
        "        tokens.append(token)\r\n",
        "        masks.append(mask)\r\n",
        "        segments.append(segment)\r\n",
        "        \r\n",
        "        # 정답label을 targets에 저장\r\n",
        "        targets.append(data_df[LABEL_COLUMN][i])\r\n",
        "\r\n",
        "    # tokens, masks, segments, targets를 numpy array로 지정    \r\n",
        "    tokens = np.array(tokens)\r\n",
        "    masks = np.array(masks)\r\n",
        "    segments = np.array(segments)\r\n",
        "    targets = np.array(targets)\r\n",
        "\r\n",
        "    return [tokens, masks, segments], targets\r\n",
        "\r\n",
        "# convert_data 함수를 불러오는 함수를 정의\r\n",
        "def load_data(pandas_dataframe):\r\n",
        "    data_df = pandas_dataframe\r\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\r\n",
        "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\r\n",
        "    data_x, data_y = convert_data(data_df)\r\n",
        "    return data_x, data_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg61oVflcsjY"
      },
      "source": [
        "# 긍부정 문장을 포함하고 있는 칼럼\r\n",
        "DATA_COLUMN = \"Sentence\"\r\n",
        "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\r\n",
        "LABEL_COLUMN = \"label\"\r\n",
        "\r\n",
        "# train 데이터를 버트 인풋에 맞게 변환\r\n",
        "train_x, train_y = load_data(train)\r\n",
        "\r\n",
        "# validation 데이터를 버트 인풋에 맞게 변환\r\n",
        "validation_x, validation_y = load_data(val)\r\n",
        "\r\n",
        "# test 데이터를 버트 인풋에 맞게 변환\r\n",
        "test_x, test_y = load_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1OtC62JaI8S"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_YlM23laHHi"
      },
      "source": [
        "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\r\n",
        "# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\r\n",
        "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\r\n",
        "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\r\n",
        "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\r\n",
        "# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\r\n",
        "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "886IbYsoaMAx"
      },
      "source": [
        "bert_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM85m8neaNzZ"
      },
      "source": [
        "bert_outputs = bert_outputs[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAnLF__laPoc"
      },
      "source": [
        "# Rectified Adam \r\n",
        "import tensorflow_addons as tfa\r\n",
        "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = ITERATIONS*EPOCHS, warmup_proportion=0.15, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IUrq_cwaPsi"
      },
      "source": [
        "sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\r\n",
        "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\r\n",
        "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\r\n",
        "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZg7TCQaPvl"
      },
      "source": [
        "sentiment_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9cRSfT1ad6f"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9zBTNnKaPyP"
      },
      "source": [
        "sentiment_model.fit(train_x, train_y, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, validation_data=(validation_x, validation_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrIeRT2iaoFI"
      },
      "source": [
        "#### prediect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWPLkV-iajEk"
      },
      "source": [
        "def predict_convert_data(data_df):\r\n",
        "    global tokenizer\r\n",
        "    tokens, masks, segments = [], [], []\r\n",
        "    \r\n",
        "    for i in tqdm(range(len(data_df))):\r\n",
        "\r\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\r\n",
        "        num_zeros = token.count(0)\r\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\r\n",
        "        segment = [0]*SEQ_LEN\r\n",
        "\r\n",
        "        tokens.append(token)\r\n",
        "        segments.append(segment)\r\n",
        "        masks.append(mask)\r\n",
        "\r\n",
        "    tokens = np.array(tokens)\r\n",
        "    masks = np.array(masks)\r\n",
        "    segments = np.array(segments)\r\n",
        "    return [tokens, masks, segments]\r\n",
        "\r\n",
        "def predict_load_data(pandas_dataframe):\r\n",
        "    data_df = pandas_dataframe\r\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\r\n",
        "    data_x = predict_convert_data(data_df)\r\n",
        "    return data_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZi11MHVauiq"
      },
      "source": [
        "#### test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_OVrtSiaqc8"
      },
      "source": [
        "test_set = predict_load_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kL8mWqrasWk"
      },
      "source": [
        "test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqmV0yZbawwF"
      },
      "source": [
        "preds = sentiment_model.predict(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qRViwtMa1D2"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "y_true = test['label']\r\n",
        "# F1 Score 확인\r\n",
        "print(classification_report(y_true, np.round(preds,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Rq7I5na4Iy"
      },
      "source": [
        "import logging\r\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaCfkio1a5Fm"
      },
      "source": [
        "#### competition data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jyt01kPa7i-"
      },
      "source": [
        "cptt_test = pd.read_csv(DATA_PATH + \"ko_data.csv\", sep=',', encoding='CP949')\r\n",
        "print(cptt_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Hg4zXTbADI"
      },
      "source": [
        "cptt_test[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA1SsQAdew9T"
      },
      "source": [
        "cptt_set = predict_load_data(cptt_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCm1hNCwe7sH"
      },
      "source": [
        "cptt_predict_label = sentiment_model.predict(cptt_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIv4lyvBfFHf"
      },
      "source": [
        "print(cptt_predict_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onhuf1Cn8Eua"
      },
      "source": [
        "df0 = pd.DataFrame(data=cptt_test['Id'], columns=['Id'])\r\n",
        "df1 = pd.DataFrame(data=np.round(cptt_predict_label,0),columns=['Predicted'])\r\n",
        "\r\n",
        "final_predicted = pd.concat([df0, df1], axis=1)\r\n",
        "final_predicted[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t20sRu7saN4g"
      },
      "source": [
        "final_predicted.to_csv(DATA_PATH + 'sample.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}